{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sparse\n",
    "import scipy.linalg as la\n",
    "\n",
    "def grad(f, x):\n",
    "    '''\n",
    "        Input:\n",
    "            f: lambda function\n",
    "            x: function args\n",
    "        Output:\n",
    "            grad_f: function gradient at x\n",
    "    '''\n",
    "    n = len(x)\n",
    "    grad_f = np.zeros(n)\n",
    "    \n",
    "    E = np.diag([pow(np.finfo(float).eps, 1/3) * (abs(a) + 1) for a in x])\n",
    "    \n",
    "    for i in range(n):\n",
    "        grad_f[i] = (f(x + E[:, i]) - f(x - E[:, i])) * (0.5 / E[i, i])\n",
    "    \n",
    "    return grad_f\n",
    "    \n",
    "def hess(f, x):\n",
    "    '''\n",
    "        Input:\n",
    "            f: lambda function\n",
    "            x: function args\n",
    "        Output:\n",
    "            hess_f: hessian of f at x\n",
    "    '''\n",
    "    n = len(x)\n",
    "    hess_f = np.zeros([n, n])\n",
    "    \n",
    "    E = np.diag([pow(np.finfo(float).eps, 1/4) * (abs(a) + 1) for a in x])\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            hess_f[i, j] = (  f(x + E[:, i] + E[:, j]) \n",
    "                            - f(x - E[:, i] + E[:, j]) \n",
    "                            - f(x + E[:, i] - E[:, j]) \n",
    "                            + f(x - E[:, i] - E[:, j]) ) * (0.25 / (E[i, i] * E[j, j]))\n",
    "    \n",
    "    return hess_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def coor_descent(f, x0, tol=1e-5, maxiter=100):\n",
    "    '''\n",
    "        Input:\n",
    "            f - func to minimize\n",
    "            x0 - initial point\n",
    "            tol - tolerance\n",
    "            maxiter - max num of iterations\n",
    "        Output:\n",
    "            xf - final aproximation of x*\n",
    "            iterations - number of iterations to reach soln\n",
    "    '''\n",
    "    \n",
    "    xf = x0\n",
    "    n = len(x0)\n",
    "    iterations = 0\n",
    "    c_1 = 0.1\n",
    "    \n",
    "    d = np.zeros(n)\n",
    "    grad_f = grad(f, xf)\n",
    "    \n",
    "    while iterations < maxiter and np.linalg.norm(grad_f, np.inf) > tol:\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Find descent direction\n",
    "        i = np.argmax(abs(grad_f))\n",
    "        d[i] = -np.sign(grad_f[i])\n",
    "        \n",
    "        #Direction coef.\n",
    "        alfa = 1\n",
    "        \n",
    "        while f(xf + alfa*d) > f(xf) - alfa*c_1*grad_f[i]:\n",
    "            alfa /= 2\n",
    "        \n",
    "        \n",
    "        #Next iteration\n",
    "        xf = xf + alfa*d\n",
    "        iterations += 1\n",
    "        d[i] = 0\n",
    "        grad_f = grad(f, xf)\n",
    "    \n",
    "    return xf, iterations\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_descent(f, x0, tol=1e-5, maxiter=100):\n",
    "    '''\n",
    "        Input:\n",
    "            f - func to minimize\n",
    "            x0 - initial point\n",
    "            tol - tolerance\n",
    "            maxiter - max num of iterations\n",
    "        Output:\n",
    "            xf - final aproximation of x*\n",
    "            iterations - number of iterations to reach soln\n",
    "    '''\n",
    "    \n",
    "    xf = x0\n",
    "    n = len(x0)\n",
    "    iterations = 0\n",
    "    c_1 = 0.1\n",
    "    \n",
    "    grad_f = grad(f, xf)\n",
    "    \n",
    "    while iterations < maxiter and np.linalg.norm(grad_f, np.inf) > tol:\n",
    "        \n",
    "        #Find descent direction\n",
    "        d = - grad_f / np.linalg.norm(grad_f)\n",
    "        \n",
    "        #Direction coef.\n",
    "        alfa = 1\n",
    "        \n",
    "        while f(xf + alfa*d) > f(xf) + alfa*c_1*np.dot(grad_f, d):\n",
    "            alfa /= 2\n",
    "        \n",
    "        #Next iteration\n",
    "        xf = xf + alfa*d\n",
    "        iterations += 1\n",
    "        grad_f = grad(f, xf)\n",
    "    \n",
    "    return xf, iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def newton_descent(f, x0, tol=1e-5, maxiter=100):\n",
    "    '''\n",
    "        Input:\n",
    "            f - func to minimize\n",
    "            x0 - initial point\n",
    "            tol - tolerance\n",
    "            maxiter - max num of iterations\n",
    "        Output:\n",
    "            xf - final aproximation of x*\n",
    "            iterations - number of iterations to reach soln\n",
    "    '''\n",
    "    \n",
    "    xf = x0\n",
    "    n = len(x0)\n",
    "    iterations = 0\n",
    "    c_1 = 0.1\n",
    "    \n",
    "    grad_f = grad(f, xf)\n",
    "    hess_f = hess(f, xf)\n",
    "    while iterations < maxiter and np.linalg.norm(grad_f, np.inf) > tol:\n",
    "        \n",
    "        #Find descent direction\n",
    "        d = np.linalg.solve(hess_f, -grad_f)\n",
    "        \n",
    "        #Direction coef.\n",
    "        alfa = 1\n",
    "        \n",
    "        while f(xf + alfa*d) > f(xf) + alfa*c_1*np.dot(grad_f, d):\n",
    "            alfa /= 2\n",
    "        \n",
    "        #Next iteration\n",
    "        xf = xf + alfa*d\n",
    "        iterations += 1\n",
    "        grad_f = grad(f, xf)\n",
    "        hess_f = hess(f, xf)\n",
    "    \n",
    "    return xf, iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Ejer 2.2.1\n",
    "'''\n",
    "A = la.pascal(4)\n",
    "b = -np.ones(4)\n",
    "f = lambda x : 1/2 * np.dot(x, np.dot(A, x)) + np.dot(b, x) + 1\n",
    "x0 = np.array([4, 4, 4, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  1.00012207e+00,  -2.74658203e-04,   2.25067139e-04,\n",
       "         -6.38961792e-05]), 1615)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coor_descent(f, x0, 1e-5, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  1.00006423e+00,  -1.50459317e-04,   1.23717900e-04,\n",
       "         -3.50451830e-05]), 553)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_descent(f, x0, 1e-5, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  9.99999999e-01,   2.25245600e-09,  -1.50217971e-09,\n",
       "          3.75111942e-10]), 1)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newton_descent(f, x0, 1e-5, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Ejer 2.2.2\n",
    "'''\n",
    "rosenbrock = lambda x : 100*(x[0]**2 - x[1])**2 + (x[0]-1)**2\n",
    "x0_r = np.array([2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.46264648,  2.140625  ]), 1000)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coor_descent(rosenbrock, x0_r, 1e-5, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.66059885,  2.75876728]), 1000)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_descent(rosenbrock, x0_r, 1e-5, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.,  1.]), 14)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newton_descent(rosenbrock, x0_r, 1e-5, 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
